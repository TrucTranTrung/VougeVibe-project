{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9P04qQN5tr2z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi3sFn_UvAo_",
        "outputId": "58324dda-951e-4c0e-b693-882c7441be9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM2ftidNteIe",
        "outputId": "c0c9bbdc-431a-4f84-e62a-af5f80e27492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n",
            "500\n",
            "510\n",
            "520\n",
            "530\n",
            "540\n",
            "550\n",
            "560\n",
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n",
            "620\n",
            "630\n",
            "640\n",
            "650\n",
            "660\n",
            "670\n",
            "680\n",
            "690\n",
            "700\n",
            "710\n",
            "720\n",
            "730\n",
            "740\n",
            "750\n",
            "760\n",
            "770\n",
            "780\n",
            "790\n",
            "800\n",
            "810\n",
            "820\n",
            "830\n",
            "840\n",
            "850\n",
            "860\n",
            "870\n",
            "880\n",
            "890\n",
            "900\n",
            "910\n",
            "920\n",
            "930\n",
            "940\n",
            "950\n",
            "960\n",
            "970\n",
            "980\n",
            "990\n",
            "1000\n",
            "1010\n",
            "1020\n",
            "1030\n",
            "1040\n",
            "1050\n",
            "1060\n",
            "1070\n",
            "1080\n",
            "1090\n",
            "1100\n",
            "1110\n",
            "1120\n",
            "1130\n",
            "1140\n",
            "1150\n",
            "1160\n",
            "1170\n",
            "1180\n",
            "1190\n",
            "1200\n",
            "Feature extraction completed and saved.\n"
          ]
        }
      ],
      "source": [
        "# 1. Load pre-trained model Resnet50\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "# 2. Function to extract features for a single image\n",
        "def extract_features(image_path, model, transform):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    return features.numpy()\n",
        "\n",
        "# 3. Transform for input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 4. Function to extract and save features for the entire dataset\n",
        "def save_features(data_dir, feature_save_path, model, transform):\n",
        "    all_features = []\n",
        "    labels = []\n",
        "    dem=0\n",
        "    for subfolder in os.listdir(data_dir):\n",
        "        subfolder_path = os.path.join(data_dir, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            image_files = [f for f in os.listdir(subfolder_path) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(subfolder_path, image_file)\n",
        "                features = extract_features(image_path, model, transform)\n",
        "                all_features.append(features)\n",
        "                dem+=1\n",
        "                if dem%10==0:\n",
        "                  print(dem)\n",
        "                labels.append(image_file)  # Save the folder name as the label for the image\n",
        "\n",
        "    all_features = np.vstack(all_features)  # Combine all features\n",
        "    np.save(feature_save_path, all_features)  # Save features to a .npy file\n",
        "    np.save('labels.npy', np.array(labels))  # Save labels to a .npy file\n",
        "\n",
        "# 5. Load features from the saved file\n",
        "def load_features(feature_save_path):\n",
        "    return np.load(feature_save_path)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/drive/MyDrive/ImagesData\"\n",
        "    feature_save_path = \"/content/saved_features.npy\"\n",
        "\n",
        "    # Load the feature extractor model\n",
        "    model = FeatureExtractor()\n",
        "    model.eval()\n",
        "\n",
        "    # Extract and save features from the database\n",
        "    save_features(data_dir, feature_save_path, model, transform)\n",
        "\n",
        "    # features = load_features(feature_save_path)\n",
        "    print(\"Feature extraction completed and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the .npy file\n",
        "data = np.load('/content/saved_features.npy')\n",
        "\n",
        "# In case you have saved labels as well\n",
        "labels = np.load('/content/labels.npy')\n",
        "\n",
        "# Print the shape of the loaded data (optional)\n",
        "print(\"Feature shape:\", data.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# Accessing the features and labels\n",
        "print(\"First feature vector:\", data[0])\n",
        "print(\"First label:\", labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnaBYn_60GXv",
        "outputId": "bc850532-779b-48f3-acaf-e8c9dc519c12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (1200, 2048)\n",
            "Labels shape: (1200,)\n",
            "First feature vector: [0.18217397 0.18432961 0.0616415  ... 0.84824365 2.4770572  0.51128876]\n",
            "First label: Headwear_26.2.jpg\n"
          ]
        }
      ]
    }
  ]
}